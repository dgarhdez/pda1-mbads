{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 17: Combining DataFrames and Data Quality\n",
    "\n",
    "In the real world, data rarely comes in a single, clean file. You'll often need to combine data from multiple sources and clean up messy data. This session covers essential techniques for data integration and quality.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "- Combine DataFrames vertically with `concat`\n",
    "- Join DataFrames horizontally with `merge`\n",
    "- Understand different types of joins\n",
    "- Convert data types with `astype`\n",
    "- Detect and handle missing values\n",
    "- Identify and handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concatenation with concat()\n",
    "\n",
    "Use `concat()` to stack DataFrames vertically (add more rows) or horizontally (add more columns).\n",
    "\n",
    "### Vertical Concatenation (Stacking Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we have sales data from different months\n",
    "january_sales = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'units': [50, 120, 80],\n",
    "    'revenue': [50000, 72000, 32000]\n",
    "})\n",
    "\n",
    "february_sales = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'units': [45, 150, 90],\n",
    "    'revenue': [45000, 90000, 36000]\n",
    "})\n",
    "\n",
    "print(\"January Sales:\")\n",
    "display(january_sales)\n",
    "\n",
    "print(\"\\nFebruary Sales:\")\n",
    "display(february_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate vertically (stack rows)\n",
    "combined = pd.concat([january_sales, february_sales])\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the index is duplicated! Reset it:\n",
    "combined = pd.concat([january_sales, february_sales], ignore_index=True)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keys to identify source\n",
    "combined_keyed = pd.concat(\n",
    "    [january_sales, february_sales], \n",
    "    keys=['January', 'February']\n",
    ")\n",
    "combined_keyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data by key\n",
    "combined_keyed.loc['January']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Mismatched Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if DataFrames have different columns?\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2],\n",
    "    'B': [3, 4]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'B': [5, 6],\n",
    "    'C': [7, 8]\n",
    "})\n",
    "\n",
    "# By default, concat creates NaN for missing columns\n",
    "combined = pd.concat([df1, df2], ignore_index=True)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use join='inner' to keep only common columns\n",
    "combined_inner = pd.concat([df1, df2], ignore_index=True, join='inner')\n",
    "combined_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Concatenation (Adding Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate horizontally with axis=1\n",
    "products = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3],\n",
    "    'name': ['Laptop', 'Phone', 'Tablet']\n",
    "})\n",
    "\n",
    "prices = pd.DataFrame({\n",
    "    'price': [1000, 600, 400],\n",
    "    'cost': [700, 350, 250]\n",
    "})\n",
    "\n",
    "combined = pd.concat([products, prices], axis=1)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging DataFrames\n",
    "\n",
    "Use `merge()` to join DataFrames based on common columns (like SQL JOINs).\n",
    "\n",
    "### Basic Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products table\n",
    "products = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3, 4],\n",
    "    'name': ['Laptop', 'Phone', 'Tablet', 'Watch'],\n",
    "    'category': ['Computers', 'Mobile', 'Mobile', 'Wearable']\n",
    "})\n",
    "\n",
    "# Orders table\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105],\n",
    "    'product_id': [1, 2, 1, 3, 2],\n",
    "    'quantity': [1, 2, 1, 3, 1],\n",
    "    'customer': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\n",
    "})\n",
    "\n",
    "print(\"Products:\")\n",
    "display(products)\n",
    "\n",
    "print(\"\\nOrders:\")\n",
    "display(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on common column (product_id)\n",
    "merged = pd.merge(orders, products, on='product_id')\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When column names differ, use left_on and right_on\n",
    "products_renamed = products.rename(columns={'product_id': 'id'})\n",
    "\n",
    "merged = pd.merge(\n",
    "    orders, \n",
    "    products_renamed, \n",
    "    left_on='product_id', \n",
    "    right_on='id'\n",
    ")\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Joins\n",
    "\n",
    "The `how` parameter controls how rows are matched:\n",
    "\n",
    "- `'inner'` (default): Only matching rows from both DataFrames\n",
    "- `'left'`: All rows from left DataFrame + matching from right\n",
    "- `'right'`: All rows from right DataFrame + matching from left\n",
    "- `'outer'`: All rows from both DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames with some non-matching values\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'dept_id': [10, 20, 10, 30, None]  # Eve has no department\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'dept_id': [10, 20, 40],  # No department 30, has unused 40\n",
    "    'dept_name': ['Engineering', 'Marketing', 'Finance']\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "display(employees)\n",
    "\n",
    "print(\"\\nDepartments:\")\n",
    "display(departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN: Only rows with matching dept_id in BOTH tables\n",
    "inner = pd.merge(employees, departments, on='dept_id', how='inner')\n",
    "print(\"INNER JOIN:\")\n",
    "inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT JOIN: All employees, with department info where available\n",
    "left = pd.merge(employees, departments, on='dept_id', how='left')\n",
    "print(\"LEFT JOIN:\")\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIGHT JOIN: All departments, with employee info where available\n",
    "right = pd.merge(employees, departments, on='dept_id', how='right')\n",
    "print(\"RIGHT JOIN:\")\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTER JOIN: All rows from both tables\n",
    "outer = pd.merge(employees, departments, on='dept_id', how='outer')\n",
    "print(\"OUTER JOIN:\")\n",
    "outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Summary of Joins\n",
    "\n",
    "```\n",
    "INNER: Only matching rows     LEFT: All from left + matches from right\n",
    "+---+---+                      +---+---+\n",
    "| A | B |                      | A | B |\n",
    "+---+---+                      +---+---+\n",
    "     ^^^                       ^^^ ^^^\n",
    "\n",
    "RIGHT: All from right + matches from left    OUTER: All from both\n",
    "+---+---+                                    +---+---+\n",
    "| A | B |                                    | A | B |\n",
    "+---+---+                                    +---+---+\n",
    "    ^^^ ^^^                                  ^^^ ^^^ ^^^\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you need to match on multiple columns\n",
    "sales_q1 = pd.DataFrame({\n",
    "    'year': [2023, 2023, 2024, 2024],\n",
    "    'quarter': ['Q1', 'Q1', 'Q1', 'Q1'],\n",
    "    'product': ['A', 'B', 'A', 'B'],\n",
    "    'sales': [100, 200, 150, 250]\n",
    "})\n",
    "\n",
    "targets = pd.DataFrame({\n",
    "    'year': [2023, 2023, 2024, 2024],\n",
    "    'quarter': ['Q1', 'Q1', 'Q1', 'Q1'],\n",
    "    'product': ['A', 'B', 'A', 'B'],\n",
    "    'target': [120, 180, 140, 220]\n",
    "})\n",
    "\n",
    "# Merge on multiple columns\n",
    "result = pd.merge(sales_q1, targets, on=['year', 'quarter', 'product'])\n",
    "result['achievement'] = (result['sales'] / result['target'] * 100).round(1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Types and astype()\n",
    "\n",
    "Correct data types are essential for analysis. Use `astype()` to convert between types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with mixed/incorrect types\n",
    "data = pd.DataFrame({\n",
    "    'id': ['1', '2', '3', '4'],  # Should be int\n",
    "    'price': ['10.99', '20.50', '15.75', '8.25'],  # Should be float\n",
    "    'quantity': [1.0, 2.0, 3.0, 1.0],  # Could be int\n",
    "    'active': ['True', 'False', 'True', 'True']  # Should be bool\n",
    "})\n",
    "\n",
    "print(\"Original types:\")\n",
    "print(data.dtypes)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert individual columns\n",
    "data['id'] = data['id'].astype(int)\n",
    "data['price'] = data['price'].astype(float)\n",
    "data['quantity'] = data['quantity'].astype(int)\n",
    "\n",
    "print(\"After conversion:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean conversion (strings 'True'/'False' don't convert directly)\n",
    "data['active'] = data['active'].map({'True': True, 'False': False})\n",
    "print(data.dtypes)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multiple columns at once\n",
    "df = pd.DataFrame({\n",
    "    'a': ['1', '2', '3'],\n",
    "    'b': ['4', '5', '6'],\n",
    "    'c': ['7.1', '8.2', '9.3']\n",
    "})\n",
    "\n",
    "df = df.astype({'a': int, 'b': int, 'c': float})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category type (memory efficient for repeated strings)\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': range(1, 1001),\n",
    "    'status': np.random.choice(['Pending', 'Shipped', 'Delivered'], 1000)\n",
    "})\n",
    "\n",
    "print(f\"Memory before: {orders['status'].memory_usage(deep=True):,} bytes\")\n",
    "\n",
    "orders['status'] = orders['status'].astype('category')\n",
    "\n",
    "print(f\"Memory after: {orders['status'].memory_usage(deep=True):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Values\n",
    "\n",
    "Missing data is represented as `NaN` (Not a Number) in Pandas. Handling missing values correctly is crucial.\n",
    "\n",
    "### Detecting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with missing values\n",
    "survey = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, None, 35, 28, None],\n",
    "    'salary': [50000, 60000, None, 55000, 48000],\n",
    "    'department': ['IT', 'HR', 'IT', None, 'Finance']\n",
    "})\n",
    "\n",
    "survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Is null (True = missing):\")\n",
    "survey.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "print(\"Missing values per column:\")\n",
    "survey.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values\n",
    "print(\"Percentage missing:\")\n",
    "(survey.isnull().sum() / len(survey) * 100).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values in the DataFrame\n",
    "print(f\"Total missing: {survey.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-null values with notnull()\n",
    "print(\"Rows where age is NOT null:\")\n",
    "survey[survey['age'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with ANY missing value\n",
    "clean = survey.dropna()\n",
    "print(\"After dropping rows with any missing value:\")\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows only if a specific column has missing values\n",
    "clean_age = survey.dropna(subset=['age'])\n",
    "print(\"After dropping rows where 'age' is missing:\")\n",
    "clean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows only if ALL values are missing\n",
    "# (First, let's add a row that's all NaN)\n",
    "survey_with_empty = pd.concat([survey, pd.DataFrame([[None]*4], columns=survey.columns)], ignore_index=True)\n",
    "print(\"With empty row:\")\n",
    "display(survey_with_empty)\n",
    "\n",
    "clean = survey_with_empty.dropna(how='all')\n",
    "print(\"\\nAfter dropping all-NaN rows:\")\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with missing values\n",
    "clean_cols = survey.dropna(axis=1)\n",
    "print(\"After dropping columns with any missing value:\")\n",
    "clean_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with a constant value\n",
    "filled = survey.fillna('Unknown')\n",
    "print(\"After filling with 'Unknown':\")\n",
    "filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill different columns with different values\n",
    "filled = survey.fillna({\n",
    "    'age': survey['age'].median(),\n",
    "    'salary': survey['salary'].mean(),\n",
    "    'department': 'Unknown'\n",
    "})\n",
    "print(\"After filling with appropriate values:\")\n",
    "filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill (use previous row's value)\n",
    "time_series = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=7),\n",
    "    'value': [100, None, None, 110, None, 115, 120]\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "display(time_series)\n",
    "\n",
    "time_series['value_ffill'] = time_series['value'].ffill()\n",
    "print(\"\\nAfter forward fill:\")\n",
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward fill (use next row's value)\n",
    "time_series['value_bfill'] = time_series['value'].bfill()\n",
    "print(\"After backward fill:\")\n",
    "time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detecting and Handling Outliers\n",
    "\n",
    "Outliers are extreme values that can skew your analysis. Let's learn how to detect and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with outliers\n",
    "np.random.seed(42)\n",
    "salaries = pd.DataFrame({\n",
    "    'employee_id': range(1, 101),\n",
    "    'salary': np.concatenate([\n",
    "        np.random.normal(50000, 10000, 95),  # Normal salaries\n",
    "        [200000, 5000, 180000, 3000, 250000]  # Outliers\n",
    "    ])\n",
    "})\n",
    "\n",
    "salaries['salary'] = salaries['salary'].round(2)\n",
    "\n",
    "print(f\"Salary statistics:\")\n",
    "print(salaries['salary'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR\n",
    "Q1 = salaries['salary'].quantile(0.25)\n",
    "Q3 = salaries['salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(f\"Q1: ${Q1:,.2f}\")\n",
    "print(f\"Q3: ${Q3:,.2f}\")\n",
    "print(f\"IQR: ${IQR:,.2f}\")\n",
    "\n",
    "# Define outlier bounds (typically 1.5 * IQR)\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nOutlier bounds: ${lower_bound:,.2f} to ${upper_bound:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers\n",
    "outliers = salaries[(salaries['salary'] < lower_bound) | (salaries['salary'] > upper_bound)]\n",
    "print(f\"Found {len(outliers)} outliers:\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "clean_salaries = salaries[(salaries['salary'] >= lower_bound) & (salaries['salary'] <= upper_bound)]\n",
    "print(f\"Original: {len(salaries)} rows\")\n",
    "print(f\"After removing outliers: {len(clean_salaries)} rows\")\n",
    "print(f\"\\nClean salary statistics:\")\n",
    "print(clean_salaries['salary'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores (how many standard deviations from mean)\n",
    "mean = salaries['salary'].mean()\n",
    "std = salaries['salary'].std()\n",
    "\n",
    "salaries['z_score'] = (salaries['salary'] - mean) / std\n",
    "\n",
    "# Outliers are typically |z| > 3\n",
    "z_outliers = salaries[abs(salaries['z_score']) > 3]\n",
    "print(f\"Outliers by Z-score (|z| > 3):\")\n",
    "z_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers: Capping/Winsorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of removing, cap outliers at bounds\n",
    "salaries_capped = salaries.copy()\n",
    "salaries_capped['salary_capped'] = salaries_capped['salary'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(\"Comparison of original vs capped (showing outliers):\")\n",
    "salaries_capped[salaries_capped['salary'] != salaries_capped['salary_capped']][['employee_id', 'salary', 'salary_capped']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Example: Cleaning a Messy Dataset\n",
    "\n",
    "Let's put it all together with a realistic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy dataset\n",
    "messy_data = pd.DataFrame({\n",
    "    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006'],\n",
    "    'age': ['25', '32', 'unknown', '28', '45', '150'],  # String, with invalid values\n",
    "    'income': [50000, None, 65000, 70000, 'N/A', 55000],  # Missing and string\n",
    "    'purchase_amount': [120.50, 89.99, 500000, 156.75, 210.00, 95.50],  # Outlier\n",
    "    'email': ['alice@email.com', 'BOB@EMAIL.COM', None, 'diana@email.com', '', 'frank@email.com']\n",
    "})\n",
    "\n",
    "print(\"Messy data:\")\n",
    "messy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check initial state\n",
    "print(\"Data types:\")\n",
    "print(messy_data.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(messy_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clean age column\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "# Convert age to numeric, replacing errors with NaN\n",
    "clean_data['age'] = pd.to_numeric(clean_data['age'], errors='coerce')\n",
    "\n",
    "# Replace unrealistic ages (>120) with NaN\n",
    "clean_data.loc[clean_data['age'] > 120, 'age'] = None\n",
    "\n",
    "# Fill missing ages with median\n",
    "clean_data['age'] = clean_data['age'].fillna(clean_data['age'].median())\n",
    "\n",
    "print(\"Age after cleaning:\")\n",
    "clean_data['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean income column\n",
    "# Replace 'N/A' with NaN\n",
    "clean_data['income'] = clean_data['income'].replace('N/A', None)\n",
    "\n",
    "# Convert to numeric\n",
    "clean_data['income'] = pd.to_numeric(clean_data['income'], errors='coerce')\n",
    "\n",
    "# Fill missing with median\n",
    "clean_data['income'] = clean_data['income'].fillna(clean_data['income'].median())\n",
    "\n",
    "print(\"Income after cleaning:\")\n",
    "clean_data['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle outliers in purchase_amount\n",
    "Q1 = clean_data['purchase_amount'].quantile(0.25)\n",
    "Q3 = clean_data['purchase_amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Cap outliers\n",
    "clean_data['purchase_amount'] = clean_data['purchase_amount'].clip(upper=upper_bound)\n",
    "\n",
    "print(\"Purchase amount after capping:\")\n",
    "clean_data['purchase_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Clean email column\n",
    "# Replace empty strings with NaN\n",
    "clean_data['email'] = clean_data['email'].replace('', None)\n",
    "\n",
    "# Standardize to lowercase\n",
    "clean_data['email'] = clean_data['email'].str.lower()\n",
    "\n",
    "print(\"Email after cleaning:\")\n",
    "clean_data['email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "print(\"Clean data:\")\n",
    "print(clean_data)\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(clean_data.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(clean_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this session, we covered:\n",
    "\n",
    "1. **concat()**: Stack DataFrames vertically or horizontally\n",
    "2. **merge()**: Join DataFrames on common columns\n",
    "3. **Types of joins**: inner, left, right, outer\n",
    "4. **astype()**: Convert data types\n",
    "5. **Missing values**: `isnull()`, `notnull()`, `dropna()`, `fillna()`\n",
    "6. **Outliers**: IQR method, Z-score, capping with `clip()`\n",
    "\n",
    "### Key Points to Remember\n",
    "\n",
    "- Use `ignore_index=True` with concat to reset the index\n",
    "- Always verify which type of join you need\n",
    "- Check data types early - wrong types cause errors\n",
    "- Decide whether to drop or fill missing values based on context\n",
    "- Document your outlier handling decisions\n",
    "\n",
    "### Next Session\n",
    "\n",
    "Practice time! We'll apply these skills to clean and merge real-world datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
