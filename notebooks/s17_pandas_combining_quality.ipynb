{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 17: Combining DataFrames and Data Quality\n",
    "\n",
    "In the real world, data rarely comes in a single, clean file. You'll often need to combine data from multiple sources and clean up messy data. This session covers essential techniques for data integration and quality.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "- Combine DataFrames vertically with `concat`\n",
    "- Join DataFrames horizontally with `merge`\n",
    "- Understand different types of joins\n",
    "- Convert data types with `astype`\n",
    "- Detect and handle missing values\n",
    "- Identify and handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concatenation with concat()\n",
    "\n",
    "Use `concat()` to stack DataFrames vertically (add more rows) or horizontally (add more columns).\n",
    "\n",
    "### Vertical Concatenation (Stacking Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imagine we have sales data from different months\njanuary_sales = pd.DataFrame({\n    'product': ['Laptop', 'Phone', 'Tablet'],\n    'units': [50, 120, 80],\n    'revenue': [50000, 72000, 32000]\n})\n\nfebruary_sales = pd.DataFrame({\n    'product': ['Laptop', 'Phone', 'Tablet'],\n    'units': [45, 150, 90],\n    'revenue': [45000, 90000, 36000]\n})\n\njanuary_sales, february_sales"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate vertically (stack rows)\n",
    "combined = pd.concat([january_sales, february_sales])\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the index is duplicated! Reset it:\n",
    "combined = pd.concat([january_sales, february_sales], ignore_index=True)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add keys to identify source\n",
    "combined_keyed = pd.concat(\n",
    "    [january_sales, february_sales], \n",
    "    keys=['January', 'February']\n",
    ")\n",
    "combined_keyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data by key\n",
    "combined_keyed.loc['January']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Mismatched Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if DataFrames have different columns?\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2],\n",
    "    'B': [3, 4]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'B': [5, 6],\n",
    "    'C': [7, 8]\n",
    "})\n",
    "\n",
    "# By default, concat creates NaN for missing columns\n",
    "combined = pd.concat([df1, df2], ignore_index=True)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use join='inner' to keep only common columns\n",
    "combined_inner = pd.concat([df1, df2], ignore_index=True, join='inner')\n",
    "combined_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Concatenation (Adding Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate horizontally with axis=1\n",
    "products = pd.DataFrame({\n",
    "    'product_id': [1, 2, 3],\n",
    "    'name': ['Laptop', 'Phone', 'Tablet']\n",
    "})\n",
    "\n",
    "prices = pd.DataFrame({\n",
    "    'price': [1000, 600, 400],\n",
    "    'cost': [700, 350, 250]\n",
    "})\n",
    "\n",
    "combined = pd.concat([products, prices], axis=1)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merging DataFrames\n",
    "\n",
    "Use `merge()` to join DataFrames based on common columns (like SQL JOINs).\n",
    "\n",
    "### Basic Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Products table\nproducts = pd.DataFrame({\n    'product_id': [1, 2, 3, 4],\n    'name': ['Laptop', 'Phone', 'Tablet', 'Watch'],\n    'category': ['Computers', 'Mobile', 'Mobile', 'Wearable']\n})\n\n# Orders table\norders = pd.DataFrame({\n    'order_id': [101, 102, 103, 104, 105],\n    'product_id': [1, 2, 1, 3, 2],\n    'quantity': [1, 2, 1, 3, 1],\n    'customer': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\n})\n\nproducts, orders"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on common column (product_id)\n",
    "merged = pd.merge(orders, products, on='product_id')\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When column names differ, use left_on and right_on\n",
    "products_renamed = products.rename(columns={'product_id': 'id'})\n",
    "\n",
    "merged = pd.merge(\n",
    "    orders, \n",
    "    products_renamed, \n",
    "    left_on='product_id', \n",
    "    right_on='id'\n",
    ")\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Joins\n",
    "\n",
    "The `how` parameter controls how rows are matched:\n",
    "\n",
    "- `'inner'` (default): Only matching rows from both DataFrames\n",
    "- `'left'`: All rows from left DataFrame + matching from right\n",
    "- `'right'`: All rows from right DataFrame + matching from left\n",
    "- `'outer'`: All rows from both DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create DataFrames with some non-matching values\nemployees = pd.DataFrame({\n    'emp_id': [1, 2, 3, 4, 5],\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'dept_id': [10, 20, 10, 30, None]  # Eve has no department\n})\n\ndepartments = pd.DataFrame({\n    'dept_id': [10, 20, 40],  # No department 30, has unused 40\n    'dept_name': ['Engineering', 'Marketing', 'Finance']\n})\n\nemployees, departments"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# INNER JOIN: Only rows with matching dept_id in BOTH tables\ninner = pd.merge(employees, departments, on='dept_id', how='inner')\ninner"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LEFT JOIN: All employees, with department info where available\nleft = pd.merge(employees, departments, on='dept_id', how='left')\nleft"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RIGHT JOIN: All departments, with employee info where available\nright = pd.merge(employees, departments, on='dept_id', how='right')\nright"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# OUTER JOIN: All rows from both tables\nouter = pd.merge(employees, departments, on='dept_id', how='outer')\nouter"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Summary of Joins\n",
    "\n",
    "```\n",
    "INNER: Only matching rows     LEFT: All from left + matches from right\n",
    "+---+---+                      +---+---+\n",
    "| A | B |                      | A | B |\n",
    "+---+---+                      +---+---+\n",
    "     ^^^                       ^^^ ^^^\n",
    "\n",
    "RIGHT: All from right + matches from left    OUTER: All from both\n",
    "+---+---+                                    +---+---+\n",
    "| A | B |                                    | A | B |\n",
    "+---+---+                                    +---+---+\n",
    "    ^^^ ^^^                                  ^^^ ^^^ ^^^\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you need to match on multiple columns\n",
    "sales_q1 = pd.DataFrame({\n",
    "    'year': [2023, 2023, 2024, 2024],\n",
    "    'quarter': ['Q1', 'Q1', 'Q1', 'Q1'],\n",
    "    'product': ['A', 'B', 'A', 'B'],\n",
    "    'sales': [100, 200, 150, 250]\n",
    "})\n",
    "\n",
    "targets = pd.DataFrame({\n",
    "    'year': [2023, 2023, 2024, 2024],\n",
    "    'quarter': ['Q1', 'Q1', 'Q1', 'Q1'],\n",
    "    'product': ['A', 'B', 'A', 'B'],\n",
    "    'target': [120, 180, 140, 220]\n",
    "})\n",
    "\n",
    "# Merge on multiple columns\n",
    "result = pd.merge(sales_q1, targets, on=['year', 'quarter', 'product'])\n",
    "result['achievement'] = (result['sales'] / result['target'] * 100).round(1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Types and astype()\n",
    "\n",
    "Correct data types are essential for analysis. Use `astype()` to convert between types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a DataFrame with mixed/incorrect types\ndata = pd.DataFrame({\n    'id': ['1', '2', '3', '4'],  # Should be int\n    'price': ['10.99', '20.50', '15.75', '8.25'],  # Should be float\n    'quantity': [1.0, 2.0, 3.0, 1.0],  # Could be int\n    'active': ['True', 'False', 'True', 'True']  # Should be bool\n})\n\ndata.dtypes, data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert individual columns\ndata['id'] = data['id'].astype(int)\ndata['price'] = data['price'].astype(float)\ndata['quantity'] = data['quantity'].astype(int)\n\ndata.dtypes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Boolean conversion (strings 'True'/'False' don't convert directly)\ndata['active'] = data['active'].map({'True': True, 'False': False})\ndata.dtypes, data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multiple columns at once\n",
    "df = pd.DataFrame({\n",
    "    'a': ['1', '2', '3'],\n",
    "    'b': ['4', '5', '6'],\n",
    "    'c': ['7.1', '8.2', '9.3']\n",
    "})\n",
    "\n",
    "df = df.astype({'a': int, 'b': int, 'c': float})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Category type (memory efficient for repeated strings)\norders = pd.DataFrame({\n    'order_id': range(1, 1001),\n    'status': np.random.choice(['Pending', 'Shipped', 'Delivered'], 1000)\n})\n\nmemory_before = orders['status'].memory_usage(deep=True)\n\norders['status'] = orders['status'].astype('category')\n\nmemory_after = orders['status'].memory_usage(deep=True)\n\nf\"Memory before: {memory_before:,} bytes, after: {memory_after:,} bytes\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Values\n",
    "\n",
    "Missing data is represented as `NaN` (Not a Number) in Pandas. Handling missing values correctly is crucial.\n",
    "\n",
    "### Detecting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with missing values\n",
    "survey = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, None, 35, 28, None],\n",
    "    'salary': [50000, 60000, None, 55000, 48000],\n",
    "    'department': ['IT', 'HR', 'IT', None, 'Finance']\n",
    "})\n",
    "\n",
    "survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for missing values\nsurvey.isnull()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count missing values per column\nsurvey.isnull().sum()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Percentage of missing values\n(survey.isnull().sum() / len(survey) * 100).round(1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Total missing values in the DataFrame\nsurvey.isnull().sum().sum()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for non-null values with notnull()\nsurvey[survey['age'].notnull()]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Drop rows with ANY missing value\nclean = survey.dropna()\nclean"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Drop rows only if a specific column has missing values\nclean_age = survey.dropna(subset=['age'])\nclean_age"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Drop rows only if ALL values are missing\n# (First, let's add a row that's all NaN)\nsurvey_with_empty = pd.concat([survey, pd.DataFrame([[None]*4], columns=survey.columns)], ignore_index=True)\n\n# Before and after\nsurvey_with_empty, survey_with_empty.dropna(how='all')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Drop columns with missing values\nclean_cols = survey.dropna(axis=1)\nclean_cols"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fill with a constant value\nfilled = survey.fillna('Unknown')\nfilled"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fill different columns with different values\nfilled = survey.fillna({\n    'age': survey['age'].median(),\n    'salary': survey['salary'].mean(),\n    'department': 'Unknown'\n})\nfilled"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Forward fill (use previous row's value)\ntime_series = pd.DataFrame({\n    'date': pd.date_range('2024-01-01', periods=7),\n    'value': [100, None, None, 110, None, 115, 120]\n})\n\ntime_series['value_ffill'] = time_series['value'].ffill()\ntime_series"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Backward fill (use next row's value)\ntime_series['value_bfill'] = time_series['value'].bfill()\ntime_series"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detecting and Handling Outliers\n",
    "\n",
    "Outliers are extreme values that can skew your analysis. Let's learn how to detect and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a dataset with outliers\nnp.random.seed(42)\nsalaries = pd.DataFrame({\n    'employee_id': range(1, 101),\n    'salary': np.concatenate([\n        np.random.normal(50000, 10000, 95),  # Normal salaries\n        [200000, 5000, 180000, 3000, 250000]  # Outliers\n    ])\n})\n\nsalaries['salary'] = salaries['salary'].round(2)\n\nsalaries['salary'].describe()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate IQR\nQ1 = salaries['salary'].quantile(0.25)\nQ3 = salaries['salary'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define outlier bounds (typically 1.5 * IQR)\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\nQ1, Q3, IQR, (lower_bound, upper_bound)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identify outliers\noutliers = salaries[(salaries['salary'] < lower_bound) | (salaries['salary'] > upper_bound)]\nlen(outliers), outliers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Remove outliers\nclean_salaries = salaries[(salaries['salary'] >= lower_bound) & (salaries['salary'] <= upper_bound)]\nlen(salaries), len(clean_salaries), clean_salaries['salary'].describe()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate Z-scores (how many standard deviations from mean)\nmean = salaries['salary'].mean()\nstd = salaries['salary'].std()\n\nsalaries['z_score'] = (salaries['salary'] - mean) / std\n\n# Outliers are typically |z| > 3\nz_outliers = salaries[abs(salaries['z_score']) > 3]\nz_outliers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers: Capping/Winsorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instead of removing, cap outliers at bounds\nsalaries_capped = salaries.copy()\nsalaries_capped['salary_capped'] = salaries_capped['salary'].clip(lower=lower_bound, upper=upper_bound)\n\n# Show comparison for outliers only\nsalaries_capped[salaries_capped['salary'] != salaries_capped['salary_capped']][['employee_id', 'salary', 'salary_capped']]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Example: Cleaning a Messy Dataset\n",
    "\n",
    "Let's put it all together with a realistic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a messy dataset\nmessy_data = pd.DataFrame({\n    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006'],\n    'age': ['25', '32', 'unknown', '28', '45', '150'],  # String, with invalid values\n    'income': [50000, None, 65000, 70000, 'N/A', 55000],  # Missing and string\n    'purchase_amount': [120.50, 89.99, 500000, 156.75, 210.00, 95.50],  # Outlier\n    'email': ['alice@email.com', 'BOB@EMAIL.COM', None, 'diana@email.com', '', 'frank@email.com']\n})\n\nmessy_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Check initial state\nmessy_data.dtypes, messy_data.isnull().sum()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Clean age column\nclean_data = messy_data.copy()\n\n# Convert age to numeric, replacing errors with NaN\nclean_data['age'] = pd.to_numeric(clean_data['age'], errors='coerce')\n\n# Replace unrealistic ages (>120) with NaN\nclean_data.loc[clean_data['age'] > 120, 'age'] = None\n\n# Fill missing ages with median\nclean_data['age'] = clean_data['age'].fillna(clean_data['age'].median())\n\nclean_data['age']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Clean income column\n# Replace 'N/A' with NaN\nclean_data['income'] = clean_data['income'].replace('N/A', None)\n\n# Convert to numeric\nclean_data['income'] = pd.to_numeric(clean_data['income'], errors='coerce')\n\n# Fill missing with median\nclean_data['income'] = clean_data['income'].fillna(clean_data['income'].median())\n\nclean_data['income']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Handle outliers in purchase_amount\nQ1 = clean_data['purchase_amount'].quantile(0.25)\nQ3 = clean_data['purchase_amount'].quantile(0.75)\nIQR = Q3 - Q1\n\nupper_bound = Q3 + 1.5 * IQR\n\n# Cap outliers\nclean_data['purchase_amount'] = clean_data['purchase_amount'].clip(upper=upper_bound)\n\nclean_data['purchase_amount']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Clean email column\n# Replace empty strings with NaN\nclean_data['email'] = clean_data['email'].replace('', None)\n\n# Standardize to lowercase\nclean_data['email'] = clean_data['email'].str.lower()\n\nclean_data['email']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final result\nclean_data, clean_data.dtypes, clean_data.isnull().sum()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this session, we covered:\n",
    "\n",
    "1. **concat()**: Stack DataFrames vertically or horizontally\n",
    "2. **merge()**: Join DataFrames on common columns\n",
    "3. **Types of joins**: inner, left, right, outer\n",
    "4. **astype()**: Convert data types\n",
    "5. **Missing values**: `isnull()`, `notnull()`, `dropna()`, `fillna()`\n",
    "6. **Outliers**: IQR method, Z-score, capping with `clip()`\n",
    "\n",
    "### Key Points to Remember\n",
    "\n",
    "- Use `ignore_index=True` with concat to reset the index\n",
    "- Always verify which type of join you need\n",
    "- Check data types early - wrong types cause errors\n",
    "- Decide whether to drop or fill missing values based on context\n",
    "- Document your outlier handling decisions\n",
    "\n",
    "### Next Session\n",
    "\n",
    "Practice time! We'll apply these skills to clean and merge real-world datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}